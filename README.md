# ğŸ“¦ ETL MinIO â†’ Parquet z Airflowem i Docker Compose

ZrobiÅ‚em prosty pipeline, ktÃ³ry przetwarza pliki CSV wrzucane do MinIO. Airflow co minutÄ™ sprawdza folder `input/`, konwertuje nowe pliki do Parquet i zapisuje je do `output/`.

CaÅ‚oÅ›Ä‡ dziaÅ‚a lokalnie w Dockerze â€“ wystarczy `docker-compose up` i wszystko siÄ™ odpala (MinIO + Airflow). Bez kombinowania.

---

## ğŸ”§ Jak dziaÅ‚a pipeline

1. Do MinIO wrzucam pliki CSV do `bucket1/input/`
2. Airflow co minutÄ™ uruchamia DAG
3. DAG sprawdza, ktÃ³re pliki juÅ¼ przetworzyÅ‚ (trzyma to jako zmienna w Airflowie)
4. KaÅ¼dy nowy CSV:
   - jest pobierany lokalnie,
   - konwertowany do `.parquet`,
   - wrzucany do `bucket1/output/`

---

## ğŸ³ Docker Compose

Wszystko jest opisane w `docker-compose.yml`. Odpala:
- kontener z MinIO (porty 9000 i 9001),
- kontener z Airflowem (port 8080), z `pyspark` i `boto3` doinstalowanymi z poziomu `AIRFLOW__CORE__EXECUTOR`.

Nie trzeba niczego instalowaÄ‡ lokalnie poza Dockerem.

---

## ğŸ” Uruchamianie

```bash
git clone https://github.com/twoj-login/minio-etl-airflow.git
cd minio-etl-airflow
docker-compose up

Po uruchomieniu:

    Airflow dostÄ™pny pod: http://localhost:8080 (login: airflow, hasÅ‚o: airflow)

    MinIO: http://localhost:9001 (login: minioadmin, hasÅ‚o: minioadmin)

ğŸ“ Struktura repo

.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ minio_etl_dag.py      # DAG przetwarzajÄ…cy nowe CSV
â”œâ”€â”€ docker-compose.yml        # MinIO + Airflow
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                 # Ten plik

ğŸª£ PrzykÅ‚ad struktury bucketa

Bucket: bucket1

input/
  â”œâ”€â”€ file1.csv
  â””â”€â”€ file2.csv

output/
  â”œâ”€â”€ file1.parquet
  â””â”€â”€ file2.parquet

âœï¸ Uwagi

Ten projekt zrobiÅ‚em gÅ‚Ã³wnie dla siebie, Å¼eby poÄ‡wiczyÄ‡ Airflowa i ogarnÄ…Ä‡ pipelineâ€™y z S3-kompatybilnym storageâ€™em. WyszÅ‚o prosto, ale dziaÅ‚a.

Jak ktoÅ› chce wrzucaÄ‡ pliki i mieÄ‡ je automatycznie przerobione na Parquet â€“ to wystarczy wrzuciÄ‡ do input/ i poczekaÄ‡ minutÄ™.
