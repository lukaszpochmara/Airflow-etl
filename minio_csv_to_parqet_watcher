from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from datetime import datetime
import boto3
import pandas as pd
import os

BUCKET = "bucket1"
INPUT_PREFIX = "input/"
OUTPUT_PREFIX = "output/"
TMP_DIR = "/tmp/minio_etl"
S3_CONF = dict(
    endpoint_url='http://minio:9000',
    aws_access_key_id='minioadmin',
    aws_secret_access_key='minioadmin',
    region_name='us-east-1'
)

def ensure_tmp_dir():
    os.makedirs(TMP_DIR, exist_ok=True)

def process_new_files():
    ensure_tmp_dir()
    s3 = boto3.client('s3', **S3_CONF)
    processed = Variable.get("minio_etl_processed", default_var="").split(",")
    # List all objects in input/
    resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=INPUT_PREFIX)
    keys = [item["Key"] for item in resp.get("Contents", []) if item["Key"].endswith(".csv")]
    new_files = [k for k in keys if k not in processed]
    for key in new_files:
        local_csv = os.path.join(TMP_DIR, os.path.basename(key))
        local_parquet = local_csv.replace(".csv", ".parquet")
        out_key = OUTPUT_PREFIX + os.path.basename(local_parquet)
        print(f"Pobieram {key} -> {local_csv}")
        s3.download_file(BUCKET, key, local_csv)
        df = pd.read_csv(local_csv)
        df.to_parquet(local_parquet)
        print(f"ZapisujÄ™ Parquet: {out_key}")
        s3.upload_file(local_parquet, BUCKET, out_key)
        # Dodaj do listy przetworzonych
        processed.append(key)
    Variable.set("minio_etl_processed", ",".join(processed))

default_args = {'start_date': datetime(2024, 7, 10)}

with DAG(
    'minio_input_to_parquet_watcher',
    default_args=default_args,
    schedule_interval="*/10 * * * *",  # co 10 minut
    catchup=False
) as dag:
    process = PythonOperator(
        task_id='process_new_files',
        python_callable=process_new_files
    )