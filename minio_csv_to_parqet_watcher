from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from datetime import datetime
import boto3
import pandas as pd
import hashlib

BUCKET = "bucket1"
KEY = "input/hw_200.csv"
PARQUET_KEY = "output/hw_200.parquet"
TMP_CSV = "/tmp/hw_200.csv"
TMP_PARQUET = "/tmp/hw_200.parquet"
S3_CONF = dict(
    endpoint_url='http://minio:9000',
    aws_access_key_id='minioadmin',
    aws_secret_access_key='minioadmin',
    region_name='us-east-1'
)

def is_file_updated(**context):
    s3 = boto3.client('s3', **S3_CONF)
    s3.download_file(BUCKET, KEY, TMP_CSV)
    # Wylicz hash pliku
    with open(TMP_CSV, "rb") as f:
        content = f.read()
        file_hash = hashlib.md5(content).hexdigest()
    last_hash = Variable.get("hw_200_last_hash", default_var="")
    Variable.set("hw_200_last_hash", file_hash)
    # Jeśli hash się zmienił, zwróć True (pipeline poleci dalej)
    return file_hash != last_hash

def convert_to_parquet():
    df = pd.read_csv(TMP_CSV)
    df.to_parquet(TMP_PARQUET)
    print("Saved Parquet: /tmp/hw_200.parquet")

def upload_parquet_to_minio():
    s3 = boto3.client('s3', **S3_CONF)
    s3.upload_file(TMP_PARQUET, BUCKET, PARQUET_KEY)
    print(f"Uploaded Parquet to MinIO: {BUCKET}/{PARQUET_KEY}")

default_args = {'start_date': datetime(2024, 7, 10)}

with DAG(
    'minio_csv_to_parquet_watcher',
    default_args=default_args,
    schedule_interval="*/5 * * * *",  # co 5 minut
    catchup=False
) as dag:
    check_file = PythonOperator(
        task_id='check_if_updated',
        python_callable=is_file_updated,
        provide_context=True
    )

    convert = PythonOperator(
        task_id='convert_csv_to_parquet',
        python_callable=convert_to_parquet
    )

    upload = PythonOperator(
        task_id='upload_parquet_to_minio',
        python_callable=upload_parquet_to_minio
    )

    # Uruchamiaj kolejne tylko jeśli plik się zmienił
    check_file >> convert >> upload
    convert.trigger_rule = 'all_success'
    upload.trigger_rule = 'all_success'